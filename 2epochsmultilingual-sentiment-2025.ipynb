{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6192386,"sourceType":"datasetVersion","datasetId":3554542}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Introduction**\nThis notebook provides a comprehensive, step-by-step guide to building and fine-tuning a multilingual sentiment analysis model using the `nlptown/bert-base-multilingual-uncased-sentiment`pre-trained BERT model. The pipeline includes data preprocessing, model fine-tuning with Hugging Face's Trainer, and performance evaluation. The fine-tuned model achieves a test accuracy of 77% and a test F1 score of 76%, demonstrating its effectiveness in classifying sentiment as negative, neutral, or positive. **Training used 84,000 examples and took approximately 1 hour on a T4 GPU.**","metadata":{}},{"cell_type":"markdown","source":"## **Step 1: Import Libraries**","metadata":{}},{"cell_type":"code","source":"# Install the Hugging Face `datasets` library if not already installed\n#!pip install datasets \n\n# Import the Dataset class for working with datasets in Hugging Face\nfrom datasets import Dataset \n\n# Import pandas for data manipulation and analysis\nimport pandas as pd \n\n# Import numpy for numerical computations\nimport numpy as np \n\n# Import the tokenizer and model for sequence classification from the Hugging Face Transformers library\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification \n\n# Import Trainer and TrainingArguments for fine-tuning and training models\nfrom transformers import Trainer, TrainingArguments \n\n# Import accuracy_score and precision_recall_fscore_support for custom evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support \n\n# Import EarlyStoppingCallback for stopping training early if no improvement in validation loss\nfrom transformers import EarlyStoppingCallback\n\n#Import warnings to silence warnings that are not causing issues with the model output\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.parallel\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:54:47.495812Z","iopub.execute_input":"2025-08-31T07:54:47.496105Z","iopub.status.idle":"2025-08-31T07:55:09.835023Z","shell.execute_reply.started":"2025-08-31T07:54:47.496082Z","shell.execute_reply":"2025-08-31T07:55:09.834385Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## **Step 2: Load and Process Data**\n- Load the complete training, test, and validation datasets from Kaggle.\n- Reduce the languages from six to four (English, French, Spanish, and German) for compatibility with the `nlptown/bert-base-multilingual-uncased-sentiment model`.\n- Transform the 5-star ratings into three polarity categories: **positive**, **negative**, and **neutral**.\n- Downsample the dataset from 1.26 million entries to 84,000.\n- Modify input labels to align with the tokenization process.\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### **Load the Dataset**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/amazon-reviews-multi/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/amazon-reviews-multi/test.csv\")\ndev_df = pd.read_csv(\"/kaggle/input/amazon-reviews-multi/validation.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:15.704553Z","iopub.execute_input":"2025-08-31T07:55:15.705139Z","iopub.status.idle":"2025-08-31T07:55:26.475002Z","shell.execute_reply.started":"2025-08-31T07:55:15.705109Z","shell.execute_reply":"2025-08-31T07:55:26.474281Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Print the number of examples in the training set\n# Training set has 1.2 million examples (95.2% of the total data)\nprint(f\"Training Examples: {train_df.shape[0]}\") \n\n# Print the number of examples in the testing set\n# Testing set has 30,000 examples (2.4% of the total data)\nprint(f\"Testing Examples: {test_df.shape[0]}\") \n\n# Print the number of examples in the development (validation) set\n# Development set has 30,000 examples (2.4% of the total data)\nprint(f\"Development Examples: {dev_df.shape[0]}\") \n\n# Calculate the total number of examples across all datasets\ntotal_examples = train_df.shape[0] + test_df.shape[0] + dev_df.shape[0]\nprint(f\"Total Examples: {total_examples}\")  # Should equal 1.26 million in this case\n\n# Calculate the percentage of examples in the test set\ntest_set_percentage = test_df.shape[0] / total_examples * 100\n\n# Calculate the percentage of examples in the development set\ndev_set_percentage = dev_df.shape[0] / total_examples * 100\n\n# Calculate the percentage of examples in the training set\ntrain_set_percentage = train_df.shape[0] / total_examples * 100\n\n# Print the calculated percentages for each dataset\n# Should confirm the split as approximately 95.2% training, 2.4% testing, and 2.4% development\nprint(f\"Test Set Percentage: {test_set_percentage:.1f}%\")\nprint(f\"Development Set Percentage: {dev_set_percentage:.1f}%\")\nprint(f\"Training Set Percentage: {train_set_percentage:.1f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:31.096523Z","iopub.execute_input":"2025-08-31T07:55:31.096851Z","iopub.status.idle":"2025-08-31T07:55:31.104733Z","shell.execute_reply.started":"2025-08-31T07:55:31.096814Z","shell.execute_reply":"2025-08-31T07:55:31.103437Z"}},"outputs":[{"name":"stdout","text":"Training Examples: 1200000\nTesting Examples: 30000\nDevelopment Examples: 30000\nTotal Examples: 1260000\nTest Set Percentage: 2.4%\nDevelopment Set Percentage: 2.4%\nTraining Set Percentage: 95.2%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### **Filter Target Languages**","metadata":{}},{"cell_type":"code","source":"# Define the target languages to include in the filtered datasets\ntarget_languages = ['en', 'es', 'fr', 'de']  # English, Spanish, French, German\n\n# Filter the training dataset to include only examples in the target languages\n# Result: 800,000 examples remain after filtering\ntrain_df = train_df[train_df['language'].isin(target_languages)] \n\n# Filter the development (validation) dataset to include only examples in the target languages\n# Result: 20,000 examples remain after filtering\ndev_df = dev_df[dev_df['language'].isin(target_languages)] \n\n# Filter the testing dataset to include only examples in the target languages\n# Result: 20,000 examples remain after filtering\ntest_df = test_df[test_df['language'].isin(target_languages)] \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:58.874096Z","iopub.execute_input":"2025-08-31T07:55:58.874424Z","iopub.status.idle":"2025-08-31T07:55:59.057566Z","shell.execute_reply.started":"2025-08-31T07:55:58.874395Z","shell.execute_reply":"2025-08-31T07:55:59.056831Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:43.657703Z","iopub.execute_input":"2025-08-31T07:55:43.657997Z","iopub.status.idle":"2025-08-31T07:55:43.680062Z","shell.execute_reply.started":"2025-08-31T07:55:43.657973Z","shell.execute_reply":"2025-08-31T07:55:43.679415Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0   review_id          product_id          reviewer_id  stars  \\\n0           0  de_0203609  product_de_0865382  reviewer_de_0267719      1   \n1           1  de_0559494  product_de_0678997  reviewer_de_0783625      1   \n2           2  de_0238777  product_de_0372235  reviewer_de_0911426      1   \n3           3  de_0477884  product_de_0719501  reviewer_de_0836478      1   \n4           4  de_0270868  product_de_0022613  reviewer_de_0736276      1   \n\n                                         review_body  \\\n0     Armband ist leider nach 1 Jahr kaputt gegangen   \n1                 In der Lieferung war nur Ein Akku!   \n2  Ein Stern, weil gar keine geht nicht. Es hande...   \n3  Dachte, das wären einfach etwas festere Binden...   \n4  Meine Kinder haben kaum damit gespielt und nac...   \n\n                review_title language  product_category  \n0  Leider nach 1 Jahr kaputt       de            sports  \n1   EINS statt ZWEI Akkus!!!       de  home_improvement  \n2            Achtung Abzocke       de         drugstore  \n3          Zu viel des Guten       de         drugstore  \n4     Qualität sehr schlecht       de               toy  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>review_id</th>\n      <th>product_id</th>\n      <th>reviewer_id</th>\n      <th>stars</th>\n      <th>review_body</th>\n      <th>review_title</th>\n      <th>language</th>\n      <th>product_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>de_0203609</td>\n      <td>product_de_0865382</td>\n      <td>reviewer_de_0267719</td>\n      <td>1</td>\n      <td>Armband ist leider nach 1 Jahr kaputt gegangen</td>\n      <td>Leider nach 1 Jahr kaputt</td>\n      <td>de</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>de_0559494</td>\n      <td>product_de_0678997</td>\n      <td>reviewer_de_0783625</td>\n      <td>1</td>\n      <td>In der Lieferung war nur Ein Akku!</td>\n      <td>EINS statt ZWEI Akkus!!!</td>\n      <td>de</td>\n      <td>home_improvement</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>de_0238777</td>\n      <td>product_de_0372235</td>\n      <td>reviewer_de_0911426</td>\n      <td>1</td>\n      <td>Ein Stern, weil gar keine geht nicht. Es hande...</td>\n      <td>Achtung Abzocke</td>\n      <td>de</td>\n      <td>drugstore</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>de_0477884</td>\n      <td>product_de_0719501</td>\n      <td>reviewer_de_0836478</td>\n      <td>1</td>\n      <td>Dachte, das wären einfach etwas festere Binden...</td>\n      <td>Zu viel des Guten</td>\n      <td>de</td>\n      <td>drugstore</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>de_0270868</td>\n      <td>product_de_0022613</td>\n      <td>reviewer_de_0736276</td>\n      <td>1</td>\n      <td>Meine Kinder haben kaum damit gespielt und nac...</td>\n      <td>Qualität sehr schlecht</td>\n      <td>de</td>\n      <td>toy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### **Convert Star Ratings to Sentiment Polarity**","metadata":{}},{"cell_type":"code","source":"train_df = train_df[['review_body', 'stars']]\ntest_df = test_df[['review_body', 'stars']]\ndev_df = dev_df[['review_body', 'stars']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:18.931500Z","iopub.execute_input":"2025-08-31T07:56:18.931846Z","iopub.status.idle":"2025-08-31T07:56:19.011758Z","shell.execute_reply.started":"2025-08-31T07:56:18.931813Z","shell.execute_reply":"2025-08-31T07:56:19.010835Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_df .head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:37.114854Z","iopub.execute_input":"2025-08-31T07:56:37.115190Z","iopub.status.idle":"2025-08-31T07:56:37.123557Z","shell.execute_reply.started":"2025-08-31T07:56:37.115162Z","shell.execute_reply":"2025-08-31T07:56:37.122646Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                         review_body  stars\n0     Armband ist leider nach 1 Jahr kaputt gegangen      1\n1                 In der Lieferung war nur Ein Akku!      1\n2  Ein Stern, weil gar keine geht nicht. Es hande...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_body</th>\n      <th>stars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Armband ist leider nach 1 Jahr kaputt gegangen</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In der Lieferung war nur Ein Akku!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ein Stern, weil gar keine geht nicht. Es hande...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def convert_to_polarity(stars):\n    '''\n    Converts star ratings into sentiment polarity categories.\n    Input:\n      - stars: An integer representing the star rating (1 to 5).\n    Output:\n      - 0 for negative sentiment (1 or 2 stars)\n      - 1 for neutral sentiment (3 stars)\n      - 2 for positive sentiment (4 or 5 stars)\n    '''\n    if stars in [1, 2]:\n        return 0  # Negative sentiment\n    elif stars == 3:\n        return 1  # Neutral sentiment\n    elif stars in [4, 5]:\n        return 2  # Positive sentiment\n\n# Apply the `convert_to_polarity` function to the 'stars' column in each dataset\n# This creates a new 'polarity' column representing sentiment\ntrain_df['polarity'] = train_df['stars'].apply(convert_to_polarity)  # Polarity for training data\ntest_df['polarity'] = test_df['stars'].apply(convert_to_polarity)   # Polarity for test data\ndev_df['polarity'] = dev_df['stars'].apply(convert_to_polarity)     # Polarity for validation data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:48.643872Z","iopub.execute_input":"2025-08-31T07:56:48.644169Z","iopub.status.idle":"2025-08-31T07:56:48.933013Z","shell.execute_reply.started":"2025-08-31T07:56:48.644143Z","shell.execute_reply":"2025-08-31T07:56:48.932246Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:57.051429Z","iopub.execute_input":"2025-08-31T07:56:57.051706Z","iopub.status.idle":"2025-08-31T07:56:57.060086Z","shell.execute_reply.started":"2025-08-31T07:56:57.051685Z","shell.execute_reply":"2025-08-31T07:56:57.059205Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                         review_body  stars  polarity\n0     Armband ist leider nach 1 Jahr kaputt gegangen      1         0\n1                 In der Lieferung war nur Ein Akku!      1         0\n2  Ein Stern, weil gar keine geht nicht. Es hande...      1         0\n3  Dachte, das wären einfach etwas festere Binden...      1         0\n4  Meine Kinder haben kaum damit gespielt und nac...      1         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_body</th>\n      <th>stars</th>\n      <th>polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Armband ist leider nach 1 Jahr kaputt gegangen</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In der Lieferung war nur Ein Akku!</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ein Stern, weil gar keine geht nicht. Es hande...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dachte, das wären einfach etwas festere Binden...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Meine Kinder haben kaum damit gespielt und nac...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### **Downsample the Dataset** ","metadata":{}},{"cell_type":"code","source":"# Since the datasets are incredibly large, and due to processing and memory limitations, \n# I will only use a subset of the data. The subset will still preserve the proportion of \n# samples with respect to the star ratings to maintain dataset balance.\n\n# Define the total size of the new subset\nnew_total_size = 84000  # Total number of samples to use across train, dev, and test subsets\n\n# Define the proportions for each subset (70% training, 20% development, 10% testing)\ntrain_size = int(new_total_size * 0.70)  # 70% of the total size\ndev_size = int(new_total_size * 0.20)    # 20% of the total size\ntest_size = int(new_total_size * 0.10)   # 10% of the total size\n\n# Function to create a stratified sample\ndef stratified_sample(df, target_col, sample_size):\n    \"\"\"\n    Create a stratified sample from the dataset.\n    Ensures the proportion of each category in the target column (e.g., star ratings) \n    is preserved in the sample.\n\n    Parameters:\n    - df (DataFrame): The input dataset to sample from\n    - target_col (str): The column name representing categories (e.g., 'stars')\n    - sample_size (int): The total number of samples to extract from the dataset\n\n    Returns:\n    - DataFrame: A stratified sample of the dataset with preserved proportions\n    \"\"\"\n    # Group by the target column and sample from each group based on its proportion\n    stratified_sample = df.groupby(target_col, group_keys=False).apply(\n        lambda x: x.sample(n=min(len(x), int(sample_size * len(x) / len(df))), random_state=42)\n    )\n    return stratified_sample\n\n# Perform stratified sampling for training, development, and test subsets\ntrain_subset = stratified_sample(train_df, target_col='stars', sample_size=train_size)\ndev_subset = stratified_sample(dev_df, target_col='stars', sample_size=dev_size)\ntest_subset = stratified_sample(test_df, target_col='stars', sample_size=test_size)\n\n# Verify that the star rating proportions are preserved in each subset\nprint(\"Train Subset Distribution:\")\nprint(train_subset['stars'].value_counts(normalize=True))  # Prints proportions of star ratings in training data\n\nprint(\"Dev Subset Distribution:\")\nprint(dev_subset['stars'].value_counts(normalize=True))    # Prints proportions of star ratings in development data\n\nprint(\"Test Subset Distribution:\")\nprint(test_subset['stars'].value_counts(normalize=True))   # Prints proportions of star ratings in testing data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:06:14.867255Z","iopub.execute_input":"2025-08-31T08:06:14.867607Z","iopub.status.idle":"2025-08-31T08:06:15.016114Z","shell.execute_reply.started":"2025-08-31T08:06:14.867582Z","shell.execute_reply":"2025-08-31T08:06:15.015227Z"}},"outputs":[{"name":"stdout","text":"Train Subset Distribution:\nstars\n1    0.2\n2    0.2\n3    0.2\n4    0.2\n5    0.2\nName: proportion, dtype: float64\nDev Subset Distribution:\nstars\n1    0.2\n2    0.2\n3    0.2\n4    0.2\n5    0.2\nName: proportion, dtype: float64\nTest Subset Distribution:\nstars\n1    0.2\n2    0.2\n3    0.2\n4    0.2\n5    0.2\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### **Modify Input Labels**","metadata":{}},{"cell_type":"code","source":"# Define a preprocessing function to format the data for model input\ndef preprocess_function(example):\n    \"\"\"\n    Preprocesses the input examples to create a text-label mapping.\n    \n    Parameters:\n    - example (dict): A dictionary containing 'review_body' (text) and 'polarity' (label).\n\n    Returns:\n    - dict: A dictionary with the processed text and label ready for training.\n      Keys:\n        - \"text\": The review text (from 'review_body')\n        - \"label\": The polarity label (from 'polarity')\n    \"\"\"\n    return {\n        \"text\": example[\"review_body\"],  # Map the review text\n        \"label\": example[\"polarity\"]    # Map the polarity label\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:32.629015Z","iopub.execute_input":"2025-08-31T08:07:32.629335Z","iopub.status.idle":"2025-08-31T08:07:32.633589Z","shell.execute_reply.started":"2025-08-31T08:07:32.629310Z","shell.execute_reply":"2025-08-31T08:07:32.632731Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Convert the stratified subsets into Hugging Face Dataset objects\n# Include only the 'review_body' (text) and 'polarity' (label) columns\ntrain_set = Dataset.from_pandas(train_subset[['review_body', 'polarity']])  # Training dataset\ntest_set = Dataset.from_pandas(test_subset[['review_body', 'polarity']])    # Testing dataset\ndev_set = Dataset.from_pandas(dev_subset[['review_body', 'polarity']])      # Development (validation) dataset\n\n# Preprocess the datasets by applying the preprocessing function\n# The `map` function applies the `preprocess_function` to all examples in the dataset\ntrain_set = train_set.map(preprocess_function, batched=True)  # Preprocess training dataset\ntest_set = test_set.map(preprocess_function, batched=True)    # Preprocess testing dataset\ndev_set = dev_set.map(preprocess_function, batched=True)      # Preprocess development dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:36.713918Z","iopub.execute_input":"2025-08-31T08:07:36.714217Z","iopub.status.idle":"2025-08-31T08:07:37.308675Z","shell.execute_reply.started":"2025-08-31T08:07:36.714194Z","shell.execute_reply":"2025-08-31T08:07:37.307827Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/58795 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae1b107d4c7447678f3f87374eead75b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef3906a805c4a5a8f11016128768be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4eef82143a43fe8fc5a1cb3a12d7b6"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## **Step 3: Tokenization**\n- Utilized the `nlptown/bert-base-multilingual-uncased-sentiment` model for tokenization and fine-tuning, supporting sentiment classification in English, Spanish, French, Dutch, Italian, and German.\n- Implemented a tokenization function specifically for sentiment classification.\n- Applied tokenization to the subsampled training, test, and validation datasets.","metadata":{}},{"cell_type":"markdown","source":"### **Load Pre-Trained Tokenizer**","metadata":{}},{"cell_type":"code","source":"# Specify the pre-trained model to be used for sentiment analysis\n# Using a multilingual BERT model fine-tuned for sentiment classification\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n\n# Load the tokenizer associated with the specified model\n# This tokenizer is used to tokenize the text inputs into the format required by the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:43.179835Z","iopub.execute_input":"2025-08-31T08:07:43.180122Z","iopub.status.idle":"2025-08-31T08:07:44.453002Z","shell.execute_reply.started":"2025-08-31T08:07:43.180101Z","shell.execute_reply":"2025-08-31T08:07:44.452051Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c46692409b94c63bd3eb5d9451a2f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f45c0c3a79440b2863c86b6f08b9b24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c64e81117d744b5f97d3b24b17829a23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fff7980e3a64a93af4ecdde43b29a04"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### **Define Tokenization Function** ","metadata":{}},{"cell_type":"code","source":"# Define a function to tokenize the dataset\n# The function uses the tokenizer to convert text inputs into tokenized format required by the model\ndef tokenize_function(examples):\n    \"\"\"\n    Tokenizes the input examples for the model.\n    \n    Parameters:\n    - examples (dict): A dictionary containing the \"text\" field to be tokenized.\n\n    Returns:\n    - dict: A dictionary with tokenized input features, including:\n      - input_ids: Tokenized numerical representation of the text\n      - attention_mask: Indicates which tokens are padding\n    \"\"\"\n    # Tokenize the text with the following parameters:\n    # - padding=\"max_length\": Ensures all sequences are padded to the same length (128 tokens here)\n    # - truncation=True: Truncates text that exceeds the maximum length\n    # - max_length=128: Specifies the maximum token length for each input\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:52.406186Z","iopub.execute_input":"2025-08-31T08:07:52.406530Z","iopub.status.idle":"2025-08-31T08:07:52.410640Z","shell.execute_reply.started":"2025-08-31T08:07:52.406504Z","shell.execute_reply":"2025-08-31T08:07:52.409755Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### **Tokenize Datasets**","metadata":{}},{"cell_type":"code","source":"# Apply the tokenize function to the training dataset\n# `batched=True` allows tokenization of multiple examples at once for faster processing\ntokenized_train = train_set.map(tokenize_function, batched=True)\n\n# Apply the tokenize function to the test dataset\ntokenized_test = test_set.map(tokenize_function, batched=True)\n\n# Apply the tokenize function to the development (validation) dataset\ntokenized_dev = dev_set.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:58.978313Z","iopub.execute_input":"2025-08-31T08:07:58.978885Z","iopub.status.idle":"2025-08-31T08:08:09.929597Z","shell.execute_reply.started":"2025-08-31T08:07:58.978839Z","shell.execute_reply":"2025-08-31T08:08:09.928662Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/58795 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd54dde6a3784701a5e2afea4307a746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a197653e79e946e1bf5b8bd1277b84e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c425db2499d497f8fe5dd6b2fd92a23"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## **Step 4: Define and Fine-Tune the Model**\n- Load the pre-trained model with 3 labels.\n- Define the training arguments.\n- Define the compute metrics of accurayc, f1 score, precision, and recall.\n- Train the model **a single T4 GPU takes 1 hour to train the model**","metadata":{}},{"cell_type":"markdown","source":"### **Load Pre-Trained Model**","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained model with the specified number of labels\n# - `num_labels=3`: Model output corresponds to three sentiment classes (negative, neutral, positive)\n# - `ignore_mismatched_sizes=True`: Allows adjustment if the model weights or architecture do not match perfectly\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=3, \n    ignore_mismatched_sizes=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:09.930786Z","iopub.execute_input":"2025-08-31T08:08:09.931117Z","iopub.status.idle":"2025-08-31T08:08:16.910633Z","shell.execute_reply.started":"2025-08-31T08:08:09.931084Z","shell.execute_reply":"2025-08-31T08:08:16.909739Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d731543b7cfe4efe86e6363f7a081319"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### **Define Training Arguments**","metadata":{}},{"cell_type":"code","source":"# Define the training arguments for fine-tuning the model\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",  # Directory to save the model checkpoints and results\n    evaluation_strategy=\"epoch\",  # Perform evaluation at the end of every epoch\n    save_strategy=\"epoch\",  # Save model checkpoints at the end of every epoch\n    learning_rate=1e-4,  # Initial learning rate for the optimizer\n    logging_steps=100,  # Log training progress every 100 steps\n    save_steps=100,  # Save model checkpoint every 100 steps\n    per_device_train_batch_size=64,  # Training batch size per GPU/TPU/CPU\n    per_device_eval_batch_size=64,  # Evaluation batch size per GPU/TPU/CPU\n    gradient_accumulation_steps=8,  # Accumulate gradients over 8 steps before updating model weights\n    num_train_epochs=2,  # Number of training epochs\n    weight_decay=0.05,  # Weight decay for regularization\n    load_best_model_at_end=True,  # Load the best model (based on evaluation metric) after training\n    metric_for_best_model=\"accuracy\",  # Metric used to determine the best model\n    report_to=\"none\"  # Disable reporting to external tools (e.g., WandB, TensorBoard)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:27.360462Z","iopub.execute_input":"2025-08-31T08:08:27.360778Z","iopub.status.idle":"2025-08-31T08:08:27.529567Z","shell.execute_reply.started":"2025-08-31T08:08:27.360751Z","shell.execute_reply":"2025-08-31T08:08:27.528588Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### **Define Metrics**","metadata":{}},{"cell_type":"code","source":"# Define the evaluation metrics for the model's performance\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Computes evaluation metrics for the model's predictions.\n\n    Args:\n        eval_pred (tuple): A tuple containing:\n            - logits (ndarray): The predicted raw scores (logits) from the model.\n            - labels (ndarray): The true labels for the evaluation dataset.\n\n    Returns:\n        dict: A dictionary containing accuracy, F1 score, precision, and recall.\n    \"\"\"\n    # Unpack the logits and true labels from the evaluation predictions\n    logits, labels = eval_pred\n\n    # Convert logits to predicted class indices by selecting the maximum score\n    predictions = np.argmax(logits, axis=-1)\n\n    # Compute precision, recall, and F1-score using scikit-learn\n    # 'average=\"weighted\"' ensures metrics account for label imbalance\n    # 'zero_division=1' avoids division errors when precision/recall are undefined\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted', zero_division=1\n    )\n\n    # Compute accuracy as the proportion of correct predictions\n    acc = accuracy_score(labels, predictions)\n\n    # Return a dictionary containing all calculated metrics\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:52.616616Z","iopub.execute_input":"2025-08-31T08:08:52.616915Z","iopub.status.idle":"2025-08-31T08:08:52.621599Z","shell.execute_reply.started":"2025-08-31T08:08:52.616891Z","shell.execute_reply":"2025-08-31T08:08:52.620672Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### **Initialize Trainer**","metadata":{}},{"cell_type":"code","source":"# Initialize Trainer\ntrainer = Trainer(\n    model=model,  # The pre-trained model to be fine-tuned or trained\n    args=training_args,  # Configuration for the training process (e.g., batch size, learning rate)\n    train_dataset=tokenized_train,  # Tokenized training dataset\n    eval_dataset=tokenized_dev,  # Tokenized validation dataset for evaluation\n    tokenizer=tokenizer,  # Tokenizer used for processing input text data\n    callbacks=[  # List of callbacks to monitor and adjust training\n        EarlyStoppingCallback(early_stopping_patience=1)  # Stops training early if no improvement after 1 patience epoch\n    ],\n    compute_metrics=compute_metrics  # Function to calculate evaluation metrics (e.g., accuracy, F1-score)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:56.517905Z","iopub.execute_input":"2025-08-31T08:08:56.518254Z","iopub.status.idle":"2025-08-31T08:08:57.051738Z","shell.execute_reply.started":"2025-08-31T08:08:56.518209Z","shell.execute_reply":"2025-08-31T08:08:57.051079Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### **Train the Model**","metadata":{}},{"cell_type":"code","source":"trainer.train() #takes ~1 hr to train on a single T4 GPU ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:09:01.840047Z","iopub.execute_input":"2025-08-31T08:09:01.840378Z","iopub.status.idle":"2025-08-31T08:34:43.691760Z","shell.execute_reply.started":"2025-08-31T08:09:01.840347Z","shell.execute_reply":"2025-08-31T08:34:43.690870Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [114/114 25:27, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.568518</td>\n      <td>0.754762</td>\n      <td>0.754096</td>\n      <td>0.755294</td>\n      <td>0.754762</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.563200</td>\n      <td>0.554587</td>\n      <td>0.768750</td>\n      <td>0.763242</td>\n      <td>0.759984</td>\n      <td>0.768750</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=114, training_loss=0.5533648457443505, metrics={'train_runtime': 1540.1979, 'train_samples_per_second': 76.347, 'train_steps_per_second': 0.074, 'total_flos': 7673110822847232.0, 'train_loss': 0.5533648457443505, 'epoch': 1.982608695652174})"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"## **Step 5: Evaluate the Model**\n- Evaluate F1 score and accuracy on training, test, and dev sets.\n- Save predictions to a csv file. ","metadata":{}},{"cell_type":"markdown","source":"### **Evaluate on Training and Dev Sets** ","metadata":{}},{"cell_type":"code","source":"#Evaluate accuracy of training and validation sets\n\ntrain_results = trainer.evaluate(tokenized_train)\nprint(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")  # Print train accuracy\nprint(f\"Train F1 Score: {train_results['eval_f1']:.4f}\")     # Print train F1 score\n\ndev_results = trainer.evaluate(tokenized_dev)\nprint(f\"Validation Accuracy: {dev_results['eval_accuracy']:.4f}\")  # Print validation accuracy\nprint(f\"Validation F1 Score: {dev_results['eval_f1']:.4f}\")     # Print validation F1 score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:40:31.299818Z","iopub.execute_input":"2025-08-31T08:40:31.300123Z","iopub.status.idle":"2025-08-31T08:46:19.524830Z","shell.execute_reply.started":"2025-08-31T08:40:31.300100Z","shell.execute_reply":"2025-08-31T08:46:19.524026Z"}},"outputs":[{"name":"stdout","text":"Train Accuracy: 0.8256\nTrain F1 Score: 0.8204\nValidation Accuracy: 0.7688\nValidation F1 Score: 0.7632\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### **Evaluate on Test Set** ","metadata":{}},{"cell_type":"code","source":"#Evaluate accuracy of test set\ntest_results = trainer.evaluate(tokenized_test)\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")  # Print test accuracy\nprint(f\"Test F1 Score: {test_results['eval_f1']:.4f}\")     # Print test F1 score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:46:19.525652Z","iopub.execute_input":"2025-08-31T08:46:19.525874Z","iopub.status.idle":"2025-08-31T08:46:58.237009Z","shell.execute_reply.started":"2025-08-31T08:46:19.525854Z","shell.execute_reply":"2025-08-31T08:46:58.236246Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.7718\nTest F1 Score: 0.7681\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### **Save Predictions** ","metadata":{}},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_test)\nfinal_predictions = predictions.predictions.argmax(axis=1)\n\nresults_df = pd.DataFrame({\"review_body\": test_subset['review_body'], \"predicted_polarity\": final_predictions})\nresults_df.to_csv(\"predictions.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T14:58:53.547858Z","iopub.execute_input":"2025-01-08T14:58:53.548152Z","iopub.status.idle":"2025-01-08T14:59:29.546545Z","shell.execute_reply.started":"2025-01-08T14:58:53.54813Z","shell.execute_reply":"2025-01-08T14:59:29.545706Z"}},"outputs":[],"execution_count":null}]}
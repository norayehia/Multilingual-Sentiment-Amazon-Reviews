{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6192386,"sourceType":"datasetVersion","datasetId":3554542}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Introduction**\nThis notebook provides a comprehensive, step-by-step guide to building and fine-tuning a multilingual sentiment analysis model using the `nlptown/bert-base-multilingual-uncased-sentiment`pre-trained BERT model. The pipeline includes data preprocessing, model fine-tuning with Hugging Face's Trainer, and performance evaluation. The fine-tuned model achieves a test accuracy of 77% and a test F1 score of 76%, demonstrating its effectiveness in classifying sentiment as negative, neutral, or positive. **Training used 84,000 examples and took approximately 1 hour on a T4 GPU.**","metadata":{}},{"cell_type":"markdown","source":"## **Step 1: Import Libraries**","metadata":{}},{"cell_type":"code","source":"# Install the Hugging Face `datasets` library if not already installed\n#!pip install datasets \n\n# Import the Dataset class for working with datasets in Hugging Face\nfrom datasets import Dataset \n\n# Import pandas for data manipulation and analysis\nimport pandas as pd \n\n# Import numpy for numerical computations\nimport numpy as np \n\n# Import the tokenizer and model for sequence classification from the Hugging Face Transformers library\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification \n\n# Import Trainer and TrainingArguments for fine-tuning and training models\nfrom transformers import Trainer, TrainingArguments \n\n# Import accuracy_score and precision_recall_fscore_support for custom evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support \n\n# Import EarlyStoppingCallback for stopping training early if no improvement in validation loss\nfrom transformers import EarlyStoppingCallback\n\n#Import warnings to silence warnings that are not causing issues with the model output\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.parallel\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:54:47.495812Z","iopub.execute_input":"2025-08-31T07:54:47.496105Z","iopub.status.idle":"2025-08-31T07:55:09.835023Z","shell.execute_reply.started":"2025-08-31T07:54:47.496082Z","shell.execute_reply":"2025-08-31T07:55:09.834385Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## **Step 2: Load and Process Data**\n- Load the complete training, test, and validation datasets from Kaggle.\n- Reduce the languages from six to four (English, French, Spanish, and German) for compatibility with the `nlptown/bert-base-multilingual-uncased-sentiment model`.\n- Transform the 5-star ratings into three polarity categories: **positive**, **negative**, and **neutral**.\n- Downsample the dataset from 1.26 million entries to 84,000.\n- Modify input labels to align with the tokenization process.\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### **Load the Dataset**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/amazon-reviews-multi/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/amazon-reviews-multi/test.csv\")\ndev_df = pd.read_csv(\"/kaggle/input/amazon-reviews-multi/validation.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:15.704553Z","iopub.execute_input":"2025-08-31T07:55:15.705139Z","iopub.status.idle":"2025-08-31T07:55:26.475002Z","shell.execute_reply.started":"2025-08-31T07:55:15.705109Z","shell.execute_reply":"2025-08-31T07:55:26.474281Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Print the number of examples in the training set\n# Training set has 1.2 million examples (95.2% of the total data)\nprint(f\"Training Examples: {train_df.shape[0]}\") \n\n# Print the number of examples in the testing set\n# Testing set has 30,000 examples (2.4% of the total data)\nprint(f\"Testing Examples: {test_df.shape[0]}\") \n\n# Print the number of examples in the development (validation) set\n# Development set has 30,000 examples (2.4% of the total data)\nprint(f\"Development Examples: {dev_df.shape[0]}\") \n\n# Calculate the total number of examples across all datasets\ntotal_examples = train_df.shape[0] + test_df.shape[0] + dev_df.shape[0]\nprint(f\"Total Examples: {total_examples}\")  # Should equal 1.26 million in this case\n\n# Calculate the percentage of examples in the test set\ntest_set_percentage = test_df.shape[0] / total_examples * 100\n\n# Calculate the percentage of examples in the development set\ndev_set_percentage = dev_df.shape[0] / total_examples * 100\n\n# Calculate the percentage of examples in the training set\ntrain_set_percentage = train_df.shape[0] / total_examples * 100\n\n# Print the calculated percentages for each dataset\n# Should confirm the split as approximately 95.2% training, 2.4% testing, and 2.4% development\nprint(f\"Test Set Percentage: {test_set_percentage:.1f}%\")\nprint(f\"Development Set Percentage: {dev_set_percentage:.1f}%\")\nprint(f\"Training Set Percentage: {train_set_percentage:.1f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:31.096523Z","iopub.execute_input":"2025-08-31T07:55:31.096851Z","iopub.status.idle":"2025-08-31T07:55:31.104733Z","shell.execute_reply.started":"2025-08-31T07:55:31.096814Z","shell.execute_reply":"2025-08-31T07:55:31.103437Z"}},"outputs":[{"name":"stdout","text":"Training Examples: 1200000\nTesting Examples: 30000\nDevelopment Examples: 30000\nTotal Examples: 1260000\nTest Set Percentage: 2.4%\nDevelopment Set Percentage: 2.4%\nTraining Set Percentage: 95.2%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### **Filter Target Languages**","metadata":{}},{"cell_type":"code","source":"# Define the target languages to include in the filtered datasets\ntarget_languages = ['en', 'es', 'fr', 'de']  # English, Spanish, French, German\n\n# Filter the training dataset to include only examples in the target languages\n# Result: 800,000 examples remain after filtering\ntrain_df = train_df[train_df['language'].isin(target_languages)] \n\n# Filter the development (validation) dataset to include only examples in the target languages\n# Result: 20,000 examples remain after filtering\ndev_df = dev_df[dev_df['language'].isin(target_languages)] \n\n# Filter the testing dataset to include only examples in the target languages\n# Result: 20,000 examples remain after filtering\ntest_df = test_df[test_df['language'].isin(target_languages)] \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:58.874096Z","iopub.execute_input":"2025-08-31T07:55:58.874424Z","iopub.status.idle":"2025-08-31T07:55:59.057566Z","shell.execute_reply.started":"2025-08-31T07:55:58.874395Z","shell.execute_reply":"2025-08-31T07:55:59.056831Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:55:43.657703Z","iopub.execute_input":"2025-08-31T07:55:43.657997Z","iopub.status.idle":"2025-08-31T07:55:43.680062Z","shell.execute_reply.started":"2025-08-31T07:55:43.657973Z","shell.execute_reply":"2025-08-31T07:55:43.679415Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0   review_id          product_id          reviewer_id  stars  \\\n0           0  de_0203609  product_de_0865382  reviewer_de_0267719      1   \n1           1  de_0559494  product_de_0678997  reviewer_de_0783625      1   \n2           2  de_0238777  product_de_0372235  reviewer_de_0911426      1   \n3           3  de_0477884  product_de_0719501  reviewer_de_0836478      1   \n4           4  de_0270868  product_de_0022613  reviewer_de_0736276      1   \n\n                                         review_body  \\\n0     Armband ist leider nach 1 Jahr kaputt gegangen   \n1                 In der Lieferung war nur Ein Akku!   \n2  Ein Stern, weil gar keine geht nicht. Es hande...   \n3  Dachte, das w√§ren einfach etwas festere Binden...   \n4  Meine Kinder haben kaum damit gespielt und nac...   \n\n                review_title language  product_category  \n0  Leider nach 1 Jahr kaputt       de            sports  \n1   EINS statt ZWEI Akkus!!!       de  home_improvement  \n2            Achtung Abzocke       de         drugstore  \n3          Zu viel des Guten       de         drugstore  \n4     Qualit√§t sehr schlecht       de               toy  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>review_id</th>\n      <th>product_id</th>\n      <th>reviewer_id</th>\n      <th>stars</th>\n      <th>review_body</th>\n      <th>review_title</th>\n      <th>language</th>\n      <th>product_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>de_0203609</td>\n      <td>product_de_0865382</td>\n      <td>reviewer_de_0267719</td>\n      <td>1</td>\n      <td>Armband ist leider nach 1 Jahr kaputt gegangen</td>\n      <td>Leider nach 1 Jahr kaputt</td>\n      <td>de</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>de_0559494</td>\n      <td>product_de_0678997</td>\n      <td>reviewer_de_0783625</td>\n      <td>1</td>\n      <td>In der Lieferung war nur Ein Akku!</td>\n      <td>EINS statt ZWEI Akkus!!!</td>\n      <td>de</td>\n      <td>home_improvement</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>de_0238777</td>\n      <td>product_de_0372235</td>\n      <td>reviewer_de_0911426</td>\n      <td>1</td>\n      <td>Ein Stern, weil gar keine geht nicht. Es hande...</td>\n      <td>Achtung Abzocke</td>\n      <td>de</td>\n      <td>drugstore</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>de_0477884</td>\n      <td>product_de_0719501</td>\n      <td>reviewer_de_0836478</td>\n      <td>1</td>\n      <td>Dachte, das w√§ren einfach etwas festere Binden...</td>\n      <td>Zu viel des Guten</td>\n      <td>de</td>\n      <td>drugstore</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>de_0270868</td>\n      <td>product_de_0022613</td>\n      <td>reviewer_de_0736276</td>\n      <td>1</td>\n      <td>Meine Kinder haben kaum damit gespielt und nac...</td>\n      <td>Qualit√§t sehr schlecht</td>\n      <td>de</td>\n      <td>toy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### **Convert Star Ratings to Sentiment Polarity**","metadata":{}},{"cell_type":"code","source":"train_df = train_df[['review_body', 'stars']]\ntest_df = test_df[['review_body', 'stars']]\ndev_df = dev_df[['review_body', 'stars']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:18.931500Z","iopub.execute_input":"2025-08-31T07:56:18.931846Z","iopub.status.idle":"2025-08-31T07:56:19.011758Z","shell.execute_reply.started":"2025-08-31T07:56:18.931813Z","shell.execute_reply":"2025-08-31T07:56:19.010835Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_df .head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:37.114854Z","iopub.execute_input":"2025-08-31T07:56:37.115190Z","iopub.status.idle":"2025-08-31T07:56:37.123557Z","shell.execute_reply.started":"2025-08-31T07:56:37.115162Z","shell.execute_reply":"2025-08-31T07:56:37.122646Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                         review_body  stars\n0     Armband ist leider nach 1 Jahr kaputt gegangen      1\n1                 In der Lieferung war nur Ein Akku!      1\n2  Ein Stern, weil gar keine geht nicht. Es hande...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_body</th>\n      <th>stars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Armband ist leider nach 1 Jahr kaputt gegangen</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In der Lieferung war nur Ein Akku!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ein Stern, weil gar keine geht nicht. Es hande...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def convert_to_polarity(stars):\n    '''\n    Converts star ratings into sentiment polarity categories.\n    Input:\n      - stars: An integer representing the star rating (1 to 5).\n    Output:\n      - 0 for negative sentiment (1 or 2 stars)\n      - 1 for neutral sentiment (3 stars)\n      - 2 for positive sentiment (4 or 5 stars)\n    '''\n    if stars in [1, 2]:\n        return 0  # Negative sentiment\n    elif stars == 3:\n        return 1  # Neutral sentiment\n    elif stars in [4, 5]:\n        return 2  # Positive sentiment\n\n# Apply the `convert_to_polarity` function to the 'stars' column in each dataset\n# This creates a new 'polarity' column representing sentiment\ntrain_df['polarity'] = train_df['stars'].apply(convert_to_polarity)  # Polarity for training data\ntest_df['polarity'] = test_df['stars'].apply(convert_to_polarity)   # Polarity for test data\ndev_df['polarity'] = dev_df['stars'].apply(convert_to_polarity)     # Polarity for validation data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:48.643872Z","iopub.execute_input":"2025-08-31T07:56:48.644169Z","iopub.status.idle":"2025-08-31T07:56:48.933013Z","shell.execute_reply.started":"2025-08-31T07:56:48.644143Z","shell.execute_reply":"2025-08-31T07:56:48.932246Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T07:56:57.051429Z","iopub.execute_input":"2025-08-31T07:56:57.051706Z","iopub.status.idle":"2025-08-31T07:56:57.060086Z","shell.execute_reply.started":"2025-08-31T07:56:57.051685Z","shell.execute_reply":"2025-08-31T07:56:57.059205Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                         review_body  stars  polarity\n0     Armband ist leider nach 1 Jahr kaputt gegangen      1         0\n1                 In der Lieferung war nur Ein Akku!      1         0\n2  Ein Stern, weil gar keine geht nicht. Es hande...      1         0\n3  Dachte, das w√§ren einfach etwas festere Binden...      1         0\n4  Meine Kinder haben kaum damit gespielt und nac...      1         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_body</th>\n      <th>stars</th>\n      <th>polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Armband ist leider nach 1 Jahr kaputt gegangen</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In der Lieferung war nur Ein Akku!</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ein Stern, weil gar keine geht nicht. Es hande...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dachte, das w√§ren einfach etwas festere Binden...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Meine Kinder haben kaum damit gespielt und nac...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### **Downsample the Dataset** ","metadata":{}},{"cell_type":"code","source":"# Since the datasets are incredibly large, and due to processing and memory limitations, \n# I will only use a subset of the data. The subset will still preserve the proportion of \n# samples with respect to the star ratings to maintain dataset balance.\n\n# Define the total size of the new subset\nnew_total_size = 84000  # Total number of samples to use across train, dev, and test subsets\n\n# Define the proportions for each subset (70% training, 20% development, 10% testing)\ntrain_size = int(new_total_size * 0.70)  # 70% of the total size\ndev_size = int(new_total_size * 0.20)    # 20% of the total size\ntest_size = int(new_total_size * 0.10)   # 10% of the total size\n\n# Function to create a stratified sample\ndef stratified_sample(df, target_col, sample_size):\n    \"\"\"\n    Create a stratified sample from the dataset.\n    Ensures the proportion of each category in the target column (e.g., star ratings) \n    is preserved in the sample.\n\n    Parameters:\n    - df (DataFrame): The input dataset to sample from\n    - target_col (str): The column name representing categories (e.g., 'stars')\n    - sample_size (int): The total number of samples to extract from the dataset\n\n    Returns:\n    - DataFrame: A stratified sample of the dataset with preserved proportions\n    \"\"\"\n    # Group by the target column and sample from each group based on its proportion\n    stratified_sample = df.groupby(target_col, group_keys=False).apply(\n        lambda x: x.sample(n=min(len(x), int(sample_size * len(x) / len(df))), random_state=42)\n    )\n    return stratified_sample\n\n# Perform stratified sampling for training, development, and test subsets\ntrain_subset = stratified_sample(train_df, target_col='stars', sample_size=train_size)\ndev_subset = stratified_sample(dev_df, target_col='stars', sample_size=dev_size)\ntest_subset = stratified_sample(test_df, target_col='stars', sample_size=test_size)\n\n# Verify that the star rating proportions are preserved in each subset\nprint(\"Train Subset Distribution:\")\nprint(train_subset['stars'].value_counts(normalize=True))  # Prints proportions of star ratings in training data\n\nprint(\"Dev Subset Distribution:\")\nprint(dev_subset['stars'].value_counts(normalize=True))    # Prints proportions of star ratings in development data\n\nprint(\"Test Subset Distribution:\")\nprint(test_subset['stars'].value_counts(normalize=True))   # Prints proportions of star ratings in testing data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:06:14.867255Z","iopub.execute_input":"2025-08-31T08:06:14.867607Z","iopub.status.idle":"2025-08-31T08:06:15.016114Z","shell.execute_reply.started":"2025-08-31T08:06:14.867582Z","shell.execute_reply":"2025-08-31T08:06:15.015227Z"}},"outputs":[{"name":"stdout","text":"Train Subset Distribution:\nstars\n1    0.2\n2    0.2\n3    0.2\n4    0.2\n5    0.2\nName: proportion, dtype: float64\nDev Subset Distribution:\nstars\n1    0.2\n2    0.2\n3    0.2\n4    0.2\n5    0.2\nName: proportion, dtype: float64\nTest Subset Distribution:\nstars\n1    0.2\n2    0.2\n3    0.2\n4    0.2\n5    0.2\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### **Modify Input Labels**","metadata":{}},{"cell_type":"code","source":"# Define a preprocessing function to format the data for model input\ndef preprocess_function(example):\n    \"\"\"\n    Preprocesses the input examples to create a text-label mapping.\n    \n    Parameters:\n    - example (dict): A dictionary containing 'review_body' (text) and 'polarity' (label).\n\n    Returns:\n    - dict: A dictionary with the processed text and label ready for training.\n      Keys:\n        - \"text\": The review text (from 'review_body')\n        - \"label\": The polarity label (from 'polarity')\n    \"\"\"\n    return {\n        \"text\": example[\"review_body\"],  # Map the review text\n        \"label\": example[\"polarity\"]    # Map the polarity label\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:32.629015Z","iopub.execute_input":"2025-08-31T08:07:32.629335Z","iopub.status.idle":"2025-08-31T08:07:32.633589Z","shell.execute_reply.started":"2025-08-31T08:07:32.629310Z","shell.execute_reply":"2025-08-31T08:07:32.632731Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Convert the stratified subsets into Hugging Face Dataset objects\n# Include only the 'review_body' (text) and 'polarity' (label) columns\ntrain_set = Dataset.from_pandas(train_subset[['review_body', 'polarity']])  # Training dataset\ntest_set = Dataset.from_pandas(test_subset[['review_body', 'polarity']])    # Testing dataset\ndev_set = Dataset.from_pandas(dev_subset[['review_body', 'polarity']])      # Development (validation) dataset\n\n# Preprocess the datasets by applying the preprocessing function\n# The `map` function applies the `preprocess_function` to all examples in the dataset\ntrain_set = train_set.map(preprocess_function, batched=True)  # Preprocess training dataset\ntest_set = test_set.map(preprocess_function, batched=True)    # Preprocess testing dataset\ndev_set = dev_set.map(preprocess_function, batched=True)      # Preprocess development dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:36.713918Z","iopub.execute_input":"2025-08-31T08:07:36.714217Z","iopub.status.idle":"2025-08-31T08:07:37.308675Z","shell.execute_reply.started":"2025-08-31T08:07:36.714194Z","shell.execute_reply":"2025-08-31T08:07:37.307827Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/58795 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae1b107d4c7447678f3f87374eead75b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef3906a805c4a5a8f11016128768be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4eef82143a43fe8fc5a1cb3a12d7b6"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## **Step 3: Tokenization**\n- Utilized the `nlptown/bert-base-multilingual-uncased-sentiment` model for tokenization and fine-tuning, supporting sentiment classification in English, Spanish, French, Dutch, Italian, and German.\n- Implemented a tokenization function specifically for sentiment classification.\n- Applied tokenization to the subsampled training, test, and validation datasets.","metadata":{}},{"cell_type":"markdown","source":"### **Load Pre-Trained Tokenizer**","metadata":{}},{"cell_type":"code","source":"# Specify the pre-trained model to be used for sentiment analysis\n# Using a multilingual BERT model fine-tuned for sentiment classification\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n\n# Load the tokenizer associated with the specified model\n# This tokenizer is used to tokenize the text inputs into the format required by the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:43.179835Z","iopub.execute_input":"2025-08-31T08:07:43.180122Z","iopub.status.idle":"2025-08-31T08:07:44.453002Z","shell.execute_reply.started":"2025-08-31T08:07:43.180101Z","shell.execute_reply":"2025-08-31T08:07:44.452051Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c46692409b94c63bd3eb5d9451a2f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f45c0c3a79440b2863c86b6f08b9b24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c64e81117d744b5f97d3b24b17829a23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fff7980e3a64a93af4ecdde43b29a04"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### **Define Tokenization Function** ","metadata":{}},{"cell_type":"code","source":"# Define a function to tokenize the dataset\n# The function uses the tokenizer to convert text inputs into tokenized format required by the model\ndef tokenize_function(examples):\n    \"\"\"\n    Tokenizes the input examples for the model.\n    \n    Parameters:\n    - examples (dict): A dictionary containing the \"text\" field to be tokenized.\n\n    Returns:\n    - dict: A dictionary with tokenized input features, including:\n      - input_ids: Tokenized numerical representation of the text\n      - attention_mask: Indicates which tokens are padding\n    \"\"\"\n    # Tokenize the text with the following parameters:\n    # - padding=\"max_length\": Ensures all sequences are padded to the same length (128 tokens here)\n    # - truncation=True: Truncates text that exceeds the maximum length\n    # - max_length=128: Specifies the maximum token length for each input\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:52.406186Z","iopub.execute_input":"2025-08-31T08:07:52.406530Z","iopub.status.idle":"2025-08-31T08:07:52.410640Z","shell.execute_reply.started":"2025-08-31T08:07:52.406504Z","shell.execute_reply":"2025-08-31T08:07:52.409755Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### **Tokenize Datasets**","metadata":{}},{"cell_type":"code","source":"# Apply the tokenize function to the training dataset\n# `batched=True` allows tokenization of multiple examples at once for faster processing\ntokenized_train = train_set.map(tokenize_function, batched=True)\n\n# Apply the tokenize function to the test dataset\ntokenized_test = test_set.map(tokenize_function, batched=True)\n\n# Apply the tokenize function to the development (validation) dataset\ntokenized_dev = dev_set.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:07:58.978313Z","iopub.execute_input":"2025-08-31T08:07:58.978885Z","iopub.status.idle":"2025-08-31T08:08:09.929597Z","shell.execute_reply.started":"2025-08-31T08:07:58.978839Z","shell.execute_reply":"2025-08-31T08:08:09.928662Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/58795 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd54dde6a3784701a5e2afea4307a746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a197653e79e946e1bf5b8bd1277b84e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c425db2499d497f8fe5dd6b2fd92a23"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## **Step 4: Define and Fine-Tune the Model**\n- Load the pre-trained model with 3 labels.\n- Define the training arguments.\n- Define the compute metrics of accurayc, f1 score, precision, and recall.\n- Train the model **a single T4 GPU takes 1 hour to train the model**","metadata":{}},{"cell_type":"markdown","source":"### **Load Pre-Trained Model**","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained model with the specified number of labels\n# - `num_labels=3`: Model output corresponds to three sentiment classes (negative, neutral, positive)\n# - `ignore_mismatched_sizes=True`: Allows adjustment if the model weights or architecture do not match perfectly\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=3, \n    ignore_mismatched_sizes=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:09.930786Z","iopub.execute_input":"2025-08-31T08:08:09.931117Z","iopub.status.idle":"2025-08-31T08:08:16.910633Z","shell.execute_reply.started":"2025-08-31T08:08:09.931084Z","shell.execute_reply":"2025-08-31T08:08:16.909739Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d731543b7cfe4efe86e6363f7a081319"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### **Define Training Arguments**","metadata":{}},{"cell_type":"code","source":"# Define the training arguments for fine-tuning the model\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",  # Directory to save the model checkpoints and results\n    evaluation_strategy=\"epoch\",  # Perform evaluation at the end of every epoch\n    save_strategy=\"epoch\",  # Save model checkpoints at the end of every epoch\n    learning_rate=1e-4,  # Initial learning rate for the optimizer\n    logging_steps=100,  # Log training progress every 100 steps\n    save_steps=100,  # Save model checkpoint every 100 steps\n    per_device_train_batch_size=64,  # Training batch size per GPU/TPU/CPU\n    per_device_eval_batch_size=64,  # Evaluation batch size per GPU/TPU/CPU\n    gradient_accumulation_steps=8,  # Accumulate gradients over 8 steps before updating model weights\n    num_train_epochs=2,  # Number of training epochs\n    weight_decay=0.05,  # Weight decay for regularization\n    load_best_model_at_end=True,  # Load the best model (based on evaluation metric) after training\n    metric_for_best_model=\"accuracy\",  # Metric used to determine the best model\n    report_to=\"none\"  # Disable reporting to external tools (e.g., WandB, TensorBoard)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:27.360462Z","iopub.execute_input":"2025-08-31T08:08:27.360778Z","iopub.status.idle":"2025-08-31T08:08:27.529567Z","shell.execute_reply.started":"2025-08-31T08:08:27.360751Z","shell.execute_reply":"2025-08-31T08:08:27.528588Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### **Define Metrics**","metadata":{}},{"cell_type":"code","source":"# Define the evaluation metrics for the model's performance\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Computes evaluation metrics for the model's predictions.\n\n    Args:\n        eval_pred (tuple): A tuple containing:\n            - logits (ndarray): The predicted raw scores (logits) from the model.\n            - labels (ndarray): The true labels for the evaluation dataset.\n\n    Returns:\n        dict: A dictionary containing accuracy, F1 score, precision, and recall.\n    \"\"\"\n    # Unpack the logits and true labels from the evaluation predictions\n    logits, labels = eval_pred\n\n    # Convert logits to predicted class indices by selecting the maximum score\n    predictions = np.argmax(logits, axis=-1)\n\n    # Compute precision, recall, and F1-score using scikit-learn\n    # 'average=\"weighted\"' ensures metrics account for label imbalance\n    # 'zero_division=1' avoids division errors when precision/recall are undefined\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted', zero_division=1\n    )\n\n    # Compute accuracy as the proportion of correct predictions\n    acc = accuracy_score(labels, predictions)\n\n    # Return a dictionary containing all calculated metrics\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:52.616616Z","iopub.execute_input":"2025-08-31T08:08:52.616915Z","iopub.status.idle":"2025-08-31T08:08:52.621599Z","shell.execute_reply.started":"2025-08-31T08:08:52.616891Z","shell.execute_reply":"2025-08-31T08:08:52.620672Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### **Initialize Trainer**","metadata":{}},{"cell_type":"code","source":"# Initialize Trainer\ntrainer = Trainer(\n    model=model,  # The pre-trained model to be fine-tuned or trained\n    args=training_args,  # Configuration for the training process (e.g., batch size, learning rate)\n    train_dataset=tokenized_train,  # Tokenized training dataset\n    eval_dataset=tokenized_dev,  # Tokenized validation dataset for evaluation\n    tokenizer=tokenizer,  # Tokenizer used for processing input text data\n    callbacks=[  # List of callbacks to monitor and adjust training\n        EarlyStoppingCallback(early_stopping_patience=1)  # Stops training early if no improvement after 1 patience epoch\n    ],\n    compute_metrics=compute_metrics  # Function to calculate evaluation metrics (e.g., accuracy, F1-score)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:08:56.517905Z","iopub.execute_input":"2025-08-31T08:08:56.518254Z","iopub.status.idle":"2025-08-31T08:08:57.051738Z","shell.execute_reply.started":"2025-08-31T08:08:56.518209Z","shell.execute_reply":"2025-08-31T08:08:57.051079Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### **Train the Model**","metadata":{}},{"cell_type":"code","source":"trainer.train() #takes ~1 hr to train on a single T4 GPU ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:09:01.840047Z","iopub.execute_input":"2025-08-31T08:09:01.840378Z","iopub.status.idle":"2025-08-31T08:34:43.691760Z","shell.execute_reply.started":"2025-08-31T08:09:01.840347Z","shell.execute_reply":"2025-08-31T08:34:43.690870Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [114/114 25:27, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.568518</td>\n      <td>0.754762</td>\n      <td>0.754096</td>\n      <td>0.755294</td>\n      <td>0.754762</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.563200</td>\n      <td>0.554587</td>\n      <td>0.768750</td>\n      <td>0.763242</td>\n      <td>0.759984</td>\n      <td>0.768750</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=114, training_loss=0.5533648457443505, metrics={'train_runtime': 1540.1979, 'train_samples_per_second': 76.347, 'train_steps_per_second': 0.074, 'total_flos': 7673110822847232.0, 'train_loss': 0.5533648457443505, 'epoch': 1.982608695652174})"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"## **Step 5: Evaluate the Model**\n- Evaluate F1 score and accuracy on training, test, and dev sets.\n- Save predictions to a csv file. ","metadata":{}},{"cell_type":"markdown","source":"### **Evaluate on Training and Dev Sets** ","metadata":{}},{"cell_type":"code","source":"#Evaluate accuracy of training and validation sets\n\ntrain_results = trainer.evaluate(tokenized_train)\nprint(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")  # Print train accuracy\nprint(f\"Train F1 Score: {train_results['eval_f1']:.4f}\")     # Print train F1 score\n\ndev_results = trainer.evaluate(tokenized_dev)\nprint(f\"Validation Accuracy: {dev_results['eval_accuracy']:.4f}\")  # Print validation accuracy\nprint(f\"Validation F1 Score: {dev_results['eval_f1']:.4f}\")     # Print validation F1 score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:40:31.299818Z","iopub.execute_input":"2025-08-31T08:40:31.300123Z","iopub.status.idle":"2025-08-31T08:46:19.524830Z","shell.execute_reply.started":"2025-08-31T08:40:31.300100Z","shell.execute_reply":"2025-08-31T08:46:19.524026Z"}},"outputs":[{"name":"stdout","text":"Train Accuracy: 0.8256\nTrain F1 Score: 0.8204\nValidation Accuracy: 0.7688\nValidation F1 Score: 0.7632\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### **Evaluate on Test Set** ","metadata":{}},{"cell_type":"code","source":"#Evaluate accuracy of test set\ntest_results = trainer.evaluate(tokenized_test)\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")  # Print test accuracy\nprint(f\"Test F1 Score: {test_results['eval_f1']:.4f}\")     # Print test F1 score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T08:46:19.525652Z","iopub.execute_input":"2025-08-31T08:46:19.525874Z","iopub.status.idle":"2025-08-31T08:46:58.237009Z","shell.execute_reply.started":"2025-08-31T08:46:19.525854Z","shell.execute_reply":"2025-08-31T08:46:58.236246Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.7718\nTest F1 Score: 0.7681\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### **Save Predictions** ","metadata":{}},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_test)\nfinal_predictions = predictions.predictions.argmax(axis=1)\n\nresults_df = pd.DataFrame({\"review_body\": test_subset['review_body'], \"predicted_polarity\": final_predictions})\nresults_df.to_csv(\"predictions.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T14:58:53.547858Z","iopub.execute_input":"2025-01-08T14:58:53.548152Z","iopub.status.idle":"2025-01-08T14:59:29.546545Z","shell.execute_reply.started":"2025-01-08T14:58:53.54813Z","shell.execute_reply":"2025-01-08T14:59:29.545706Z"}},"outputs":[],"execution_count":null}]}